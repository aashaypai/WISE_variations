{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "5b4757d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib\n",
    "import csv\n",
    "from matplotlib import path\n",
    "try:\n",
    "    from astropy.convolution import Gaussian2DKernel, convolve\n",
    "    astro_smooth = True\n",
    "except ImportError as IE:\n",
    "    astro_smooth = False\n",
    "\n",
    "from astropy.io import fits\n",
    "from astropy.coordinates import match_coordinates_sky as coords\n",
    "from astropy.coordinates import SkyCoord  # High-level coordinates\n",
    "from astropy.coordinates import ICRS, Galactic, FK4, FK5  # Low-level frames\n",
    "from astropy.coordinates import Angle, Latitude, Longitude  # Angles\n",
    "import astropy.units as u\n",
    "#from astropy.units import cds\n",
    "#cds.enable()\n",
    "\n",
    "from astropy.cosmology import LambdaCDM, FlatLambdaCDM\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from astropy.io import ascii\n",
    "from astropy.io.ascii import masked\n",
    "from astropy.table import Table\n",
    "\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.stats import gaussian_kde\n",
    "from numpy import polyfit\n",
    "from astropy.time import Time\n",
    "\n",
    "import uncertainties as unc  \n",
    "import uncertainties.unumpy as unp\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "a7f6b346",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_manga():\n",
    "    manga_file = fits.open(r\"C:\\Users\\paiaa\\Blanton Lab\\mnsa-0.3.0.fits\")\n",
    "    hdu_manga = manga_file[2]\n",
    "    manga_data = hdu_manga.data\n",
    "    return manga_data\n",
    "\n",
    "def write_query_table():\n",
    "    RA = manga_file[1].data['objra']\n",
    "    dec = manga_file[1].data['objdec']\n",
    "    \n",
    "    table = Table([RA, dec], names=('ra', 'dec'))\n",
    "    table.write('objects_all.tbl', format = 'ipac', overwrite = True)\n",
    "    return table\n",
    "\n",
    "#spitzer_file = fits.open(r\"C:\\Users\\paiaa\\Downloads\\asu (2).fit\")\n",
    "#pipe3d_file = fits.open(r\"C:\\Users\\paiaa\\Downloads\\SDSS17Pipe3D_v3_1_1.fits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "694bff5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hdu_p3d = pipe3d_file[1]\n",
    "#hdu_spitzer = spitzer_file[1]\n",
    "\n",
    "\n",
    "#p3d_data = hdu_p3d.data\n",
    "#spitzer_data = hdu_spitzer.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "f80c3c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hdu_manga.header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "484ce3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#csv with 37 objects\n",
    "#neowise_data = pd.read_csv(r'C:\\Users\\paiaa\\Blanton Lab\\WISE variations\\table_irsa_neowise_catalog_search_results.csv')\n",
    "#allsky_data = pd.read_csv(r'C:\\Users\\paiaa\\Blanton Lab\\WISE variations\\table_irsa_wise_allsky_catalog_search_results.csv')\n",
    "\n",
    "#csv with all MaNGA objects\n",
    "neowise_data = pd.read_csv(r'C:\\Users\\paiaa\\Blanton Lab\\WISE variations\\neowise_2arcsec_catalog_search_results_all.csv')\n",
    "allsky_data = pd.read_csv(r'C:\\Users\\paiaa\\Blanton Lab\\WISE variations\\allsky_2arcsec_catalog_search_results_all.csv')\n",
    "wise_data = pd.concat([allsky_data, neowise_data], axis = 0)\n",
    "#wise_data2 = pd.concat([allsky_data2, neowise_data2], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "cb2bc9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#processed data (using the process_df function) of all MaNGA objects\n",
    "#processed_wise_data = pd.read_csv('manga_wise_data.csv')\n",
    "#wise_avg = pd.read_csv('manga_wise_avg_data.csv')\n",
    "#wise_var = pd.read_csv('manga_wise_var_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c106fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5ca1db46",
   "metadata": {},
   "source": [
    "# functions to process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "270e773f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#global variables\n",
    "ALL_PAD_LENGTH = 26\n",
    "PER_PAD_LENGTH = 11273\n",
    "MANGA_DATA = import_manga()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "977fa068",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort(df):\n",
    "    df = df.sort_values(by=['cntr_01', 'mjd'], ascending = [True, True])\n",
    "    return df\n",
    "\n",
    "def add_columns(df):\n",
    "    #datetime column to sort by date\n",
    "    df['date'] = pd.to_datetime(df['mjd'].to_numpy() + 2400000.5, origin='julian', unit='D')\n",
    "    \n",
    "    #propagating uncertainties to W1 and W2 with the uncertainties package\n",
    "    w1mpro = unp.uarray(df['w1mpro'], df['w1sigmpro'])\n",
    "    w2mpro = unp.uarray(df['w2mpro'], df['w2sigmpro'])\n",
    "    w12 = w1mpro-w2mpro\n",
    "    w12sig = unp.std_devs(w12)\n",
    "    df['w12sigmpro (error propagated)'] = w12sig\n",
    "    df['w1mpro-w2mpro (error propagated)'] = unp.nominal_values(w12)\n",
    "    \n",
    "    \n",
    "    df['error squared'] = np.square(df['w2sigmpro'])\n",
    "    w2sig = np.square(w2mpro)\n",
    "    df['error squared (error propagated)'] = unp.std_devs(w2sig)\n",
    "    \n",
    "    #replacing NaN values with -9999\n",
    "    #df = df.fillna(-9999)\n",
    "    return df\n",
    "\n",
    "def filter_data(df):\n",
    "    df = df[df['qual_frame'] != 0]\n",
    "    df = df[df['cc_flags'] == '0000']\n",
    "    df = df[df['sigra'] < 1]\n",
    "    df = df[df['sigdec'] < 1]\n",
    "    return df\n",
    "\n",
    "def mean_var_data(df, freq):\n",
    "    avg = pd.DataFrame()\n",
    "    var = pd.DataFrame()\n",
    "    epoch_count = pd.DataFrame()\n",
    "    epoch_count = pd.DataFrame()\n",
    "    plateifus = np.array([])\n",
    "    for i in range(df['cntr_01'].max()):\n",
    "        \n",
    "        if i % 2500 == 0:\n",
    "            print('processing object ' + str(i) + ' out of ' + str(df['cntr_01'].max()))\n",
    "        \n",
    "        objects = df[df['cntr_01'] == i + 1]\n",
    "        \n",
    "        #calculating mean dataframe\n",
    "        temp1 = objects.groupby(pd.Grouper(key = 'date', freq = freq), dropna = False).mean(numeric_only = True).reset_index()\n",
    "        avg = pd.concat([avg, temp1])\n",
    "        #calculating variance dataframe\n",
    "        temp2 = objects.groupby(pd.Grouper(key = 'date', freq = freq), dropna = False).var(numeric_only = True).reset_index()\n",
    "        var = pd.concat([var, temp2])\n",
    "        \n",
    "        temp3 = objects.groupby(pd.Grouper(key = 'date', freq = freq), dropna = False).size().reset_index(name = 'count')\n",
    "        epoch_count = pd.concat([epoch_count, temp3])\n",
    "        \n",
    "        #adding plateifu column\n",
    "        plateifu = np.repeat(MANGA_DATA['plateifu'][i], objects.shape[0])\n",
    "        plateifus = np.append(plateifus, plateifu)\n",
    "        \n",
    "        #fixing cntr_01 column\n",
    "        avg['cntr_01'] = avg['cntr_01'].fillna(method = 'bfill')\n",
    "        var['cntr_01'] = avg['cntr_01']\n",
    "    avg['epoch count'] = epoch_count['count']\n",
    "    df['plateifu'] = plateifus   \n",
    "        \n",
    "    return df, avg, var\n",
    "\n",
    "def calculate_epoch(df, freq):\n",
    "    df['epoch'] = df.groupby([ pd.Grouper(key = 'date', freq = freq)]).ngroup()\n",
    "    \n",
    "    return df\n",
    "\n",
    "def pad_data(stat_data, pad_length):\n",
    "    pad_length = pad_length\n",
    "    padded_data = []\n",
    "    \n",
    "    for array in stat_data:\n",
    "        #print(array)\n",
    "        if array.shape[0] < pad_length:\n",
    "            array = np.pad(array, pad_width = (0, int(pad_length - array.shape[0])), constant_values = -9999.)\n",
    "            padded_data.append(array)\n",
    "        \n",
    "        else:\n",
    "            padded_data.append(array)\n",
    "\n",
    "    return padded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "3f73de9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_df(df, freq):\n",
    "    print('**PROCESSING DATAFRAME**')\n",
    "    t1 = time.time()\n",
    "    print('\\n**dataframe recieved**')\n",
    "    result = sort(df)\n",
    "    print('**dataframe sorted**')\n",
    "    result = add_columns(result)\n",
    "    print('**relevant columns added**')\n",
    "    result = filter_data(result)\n",
    "    print('**data filtered**')\n",
    "    result, result_mean, result_var = mean_var_data(result, freq)\n",
    "    print('**mean, variance calculated**')\n",
    "    result = calculate_epoch(result, freq)\n",
    "    print('**epochs labelled**')\n",
    "    t2 = time.time()\n",
    "    print('this took: ' + str(round(t2-t1, 2)) + ' seconds')\n",
    "    return result, result_mean, result_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "30020185",
   "metadata": {},
   "outputs": [],
   "source": [
    "#processed_wise_data, wise_avg, wise_var = process_df(wise_data, '181D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d51b89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9ecb818e",
   "metadata": {},
   "source": [
    "# Statistical calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0f3c89",
   "metadata": {},
   "source": [
    "##### (d) mean magnitude at each epoch (unweighted mean), (e) expected variance at each epoch (based on catalog sigma), (f) expected variance at each epoch (based on within-epoch magnitudes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "07be11fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#(d)\n",
    "def mean_w2_per_epoch(df_avg):\n",
    "    mean_mag = []\n",
    "    #look at mean mag per epoch in mean dataframe\n",
    "    for i in range(int(df_avg['cntr_01'].max())):\n",
    "    \n",
    "        avg = df_avg[df_avg['cntr_01'] == i+1]\n",
    "        temp = avg['w2mpro'].to_numpy()\n",
    "\n",
    "        mean_mag.append(temp)\n",
    "    padded_data = pad_data(mean_mag, ALL_PAD_LENGTH)\n",
    "    return padded_data, mean_mag\n",
    "\n",
    "def mean_w1_per_epoch(df_avg):\n",
    "    mean_mag = []\n",
    "    #look at mean mag per epoch in mean dataframe\n",
    "    for i in range(int(df_avg['cntr_01'].max())):\n",
    "    \n",
    "        avg = df_avg[df_avg['cntr_01'] == i+1]\n",
    "        temp = avg['w1mpro'].to_numpy()\n",
    "\n",
    "        mean_mag.append(temp)\n",
    "    padded_data = pad_data(mean_mag, ALL_PAD_LENGTH)\n",
    "    return padded_data, mean_mag\n",
    "\n",
    "#(e) #want it to be avg of sigma squared, not square of avg sigma\n",
    "def expected_var_per_epoch_sigma(df_avg):\n",
    "    expected_var = []\n",
    "    #look at variance (sigma squared) of the mean error (sigma) at each epoch\n",
    "    for i in range(int(df_avg['cntr_01'].max())):\n",
    "    \n",
    "        avg = df_avg[df_avg['cntr_01'] == i+1]\n",
    "        temp = avg['error squared'].to_numpy()\n",
    "\n",
    "        expected_var.append(temp)\n",
    "    padded_data = pad_data(expected_var, ALL_PAD_LENGTH)\n",
    "    return padded_data, expected_var\n",
    "\n",
    "#(f)\n",
    "def expected_var_per_epoch_mags(df_var):\n",
    "    expected_var = []\n",
    "    #look at variance of magnitudes per epoch\n",
    "    for i in range(int(df_var['cntr_01'].max())):\n",
    "    \n",
    "        var = df_var[df_var['cntr_01'] == i+1]\n",
    "        temp = var['w2mpro'].to_numpy()\n",
    "\n",
    "        expected_var.append(temp)\n",
    "    padded_data = pad_data(expected_var, ALL_PAD_LENGTH)\n",
    "    return padded_data, expected_var"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46e0e7b",
   "metadata": {},
   "source": [
    "##### (a) observed variance across all epochs, (b) expected variance across all epochs (based on catalog sigma),  (c) expected variance across all epochs (based on within-epoch variances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "29971e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#(a)\n",
    "def observed_var_all(df_avg):\n",
    "    observed_var = np.array([])\n",
    "    #isolate each galaxy\n",
    "    for i in range(int(df_avg['cntr_01'].max())):\n",
    "        avg = df_avg[df_avg['cntr_01'] == i+1]\n",
    "        #look at the variance of the mean W2 across all epochs (the mean was taken across all epochs)\n",
    "        observed_var = np.append(observed_var, avg['w2mpro'].var())\n",
    "    return observed_var\n",
    "\n",
    "#(b)\n",
    "def expected_var_all_sigma(df_avg):\n",
    "    expected_var = np.array([])\n",
    "    padded, expected_var_per_epoch = expected_var_per_epoch_sigma(df_avg)\n",
    "\n",
    "    for array in expected_var_per_epoch:\n",
    "        \n",
    "        var = np.nanmean(array)/array[~np.isnan(array)].shape[0]\n",
    "        expected_var = np.append(expected_var, var)\n",
    "    return expected_var\n",
    "\n",
    "#(c)\n",
    "def expected_var_all_mags(df_var):\n",
    "    expected_var = np.array([])\n",
    "    padded, expected_var_per_epoch = expected_var_per_epoch_mags(df_var)\n",
    "    \n",
    "    for array in expected_var_per_epoch:\n",
    "        var = np.nanmean(array)/array[~np.isnan(array)].shape[0]\n",
    "        expected_var = np.append(expected_var, var)\n",
    "    \n",
    "    return expected_var\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f728274",
   "metadata": {},
   "source": [
    "##### (g) Date associated with epoch, (h) Number of good observations at each epoch, (i) Number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "f6b78110",
   "metadata": {},
   "outputs": [],
   "source": [
    "#(g)\n",
    "def date_per_epoch(df_avg):\n",
    "    dates = []\n",
    "    for i in range(int(df_avg['cntr_01'].max())):\n",
    "        objects = df_avg[df_avg['cntr_01'] == i + 1]\n",
    "        \n",
    "        t = Time(objects['date'])\n",
    "        t = t.to_value('mjd', 'float')\n",
    "        dates.append(t)\n",
    "    padded_data = pad_data(dates, ALL_PAD_LENGTH)\n",
    "    return padded_data\n",
    "\n",
    "#(i)\n",
    "def epochs_per_object(df):\n",
    "    epoch_count = np.array([])\n",
    "    for i in range(int(df['cntr_01'].max())):\n",
    "        objects = df[df['cntr_01'] == i + 1]\n",
    "        count = objects['epoch'].max()\n",
    "        epoch_count = np.append(epoch_count, count)\n",
    "        \n",
    "    return epoch_count\n",
    "\n",
    "#(h)\n",
    "def good_obs_per_epoch(df_avg):\n",
    "    obs_count = []\n",
    "    \n",
    "    for i in range(int(df_avg['cntr_01'].max())):\n",
    "    \n",
    "        objects = df_avg[df_avg['cntr_01'] == i+1]\n",
    "        obs = objects['epoch count'].to_numpy()\n",
    "\n",
    "        obs_count.append(obs)\n",
    "    padded_data = pad_data(obs_count, ALL_PAD_LENGTH)\n",
    "    return padded_data\n",
    "\n",
    "def plateifu():\n",
    "    plateifu = MANGA_DATA['plateifu']\n",
    "    return plateifu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15dd7edf",
   "metadata": {},
   "source": [
    "### Create FITS file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "8f7bc3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fits_data(df, df_avg, df_var):\n",
    "    t1 = time.time()\n",
    "    print('\\n**PERFORMING STATISTICAL CALCULATIONS**')\n",
    "    plateifus = plateifu()\n",
    "    obs_per_epoch = good_obs_per_epoch(df_avg)\n",
    "    epochs_per_obj = epochs_per_object(df)\n",
    "    epoch_date = date_per_epoch(df_avg)\n",
    "    print('**finished calculating distribution of observations**')\n",
    "    padded_mean_w1, mean_w1 = mean_w1_per_epoch(df_avg)\n",
    "    padded_mean_w2, mean_w2 = mean_w2_per_epoch(df_avg)\n",
    "    padded_exp_var_sigma, exp_var_sigma = expected_var_per_epoch_sigma(df_avg)\n",
    "    padded_exp_var_mags, exp_var_mags = expected_var_per_epoch_mags(df_var)\n",
    "    print('**finished calculating within epoch statistics**')\n",
    "    obs_var_all = observed_var_all(df_avg)\n",
    "    exp_var_sig_all = expected_var_all_sigma(df_avg)\n",
    "    exp_var_mags_all = expected_var_all_mags(df_var)\n",
    "    print('**finished calculating epoch to epoch statistics**')\n",
    "    t2 = time.time()\n",
    "    print('this took: ' + str(round(t2-t1, 2)) + ' seconds')\n",
    "    stat_data = [plateifus, obs_per_epoch, epochs_per_obj, epoch_date, padded_mean_w1, padded_mean_w2, padded_exp_var_sigma, padded_exp_var_mags, obs_var_all, exp_var_sig_all, exp_var_mags_all]\n",
    "    #stat_data = np.nan_to_num(stat_data, nan = -9999.)\n",
    "    return stat_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "da862bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fits_file(stat_data):\n",
    "    print('\\n**CREATING FITS HDU**')\n",
    "    names = np.array(['plateIFU', 'obs per epoch', 'epochs per obj', 'epoch date', 'mean W1 per epoch', 'mean W2 per epoch', 'expected var (errors)', 'expected var (mags)', 'observed var', 'expected var all epochs (errors)', 'expected var all epochs (mags)'])\n",
    "    formats = np.array(['12A', '26K', 'D', '26D', '26D', '26D', '26D', '26D', 'D', 'D', 'D'])\n",
    "    hdu = fits.PrimaryHDU()\n",
    "    cols = np.array([])\n",
    "    for i, name in enumerate(names):\n",
    "        \n",
    "        column = fits.Column(name = name, array =  stat_data[i], format = formats[i])\n",
    "        cols = np.append(cols, column)\n",
    "    hdu = fits.BinTableHDU.from_columns(cols)\n",
    "    #print(hdu.header)\n",
    "    return hdu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "81e63909",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(hdu, processed_wise_data, wise_avg, wise_var):\n",
    "    print('\\n**SAVING DATA**')\n",
    "    hdu.writeto('WISE variations/wise_statistics_2arcsec_sigradec.fits', overwrite=True)\n",
    "    processed_wise_data.to_csv('WISE variations/processed_manga_data_2arcsec_sigradec.csv')\n",
    "    wise_avg.to_csv('WISE variations/manga_avg_data_2arcsec_sigradec.csv')\n",
    "    wise_var.to_csv('WISE variations/manga_var_data_2arcsec_sigradec.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5797f143",
   "metadata": {},
   "source": [
    "## Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "19d98b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(freq, save = True):\n",
    "    t1 = time.time()\n",
    "    processed_wise_data, wise_avg, wise_var = process_df(wise_data, freq)\n",
    "    stat_data = create_fits_data(processed_wise_data, wise_avg, wise_var)\n",
    "    hdu = create_fits_file(stat_data)\n",
    "    \n",
    "    if save == True:\n",
    "        save_data(hdu, processed_wise_data, wise_avg, wise_var)\n",
    "    \n",
    "    t2 = time.time()\n",
    "    print('the entire process took: ' + str(round(t2-t1, 2)) + ' seconds')\n",
    "    \n",
    "    return hdu, processed_wise_data, wise_avg, wise_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "725a2873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**PROCESSING DATAFRAME**\n",
      "\n",
      "**dataframe recieved**\n",
      "**dataframe sorted**\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paiaa\\anaconda3\\Lib\\site-packages\\numpy\\lib\\function_base.py:2412: RuntimeWarning: invalid value encountered in <lambda> (vectorized)\n",
      "  outputs = ufunc(*inputs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**relevant columns added**\n",
      "**data filtered**\n",
      "processing object 0 out of 11273\n",
      "processing object 2500 out of 11273\n",
      "processing object 5000 out of 11273\n",
      "processing object 7500 out of 11273\n",
      "processing object 10000 out of 11273\n",
      "**mean, variance calculated**\n",
      "**epochs labelled**\n",
      "this took: 902.77 seconds\n",
      "\n",
      "**PERFORMING STATISTICAL CALCULATIONS**\n",
      "**finished calculating distribution of observations**\n",
      "**finished calculating within epoch statistics**\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paiaa\\AppData\\Local\\Temp\\ipykernel_10236\\184507825.py:18: RuntimeWarning: Mean of empty slice\n",
      "  var = np.nanmean(array)/array[~np.isnan(array)].shape[0]\n",
      "C:\\Users\\paiaa\\AppData\\Local\\Temp\\ipykernel_10236\\184507825.py:28: RuntimeWarning: Mean of empty slice\n",
      "  var = np.nanmean(array)/array[~np.isnan(array)].shape[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**finished calculating epoch to epoch statistics**\n",
      "this took: 69.0 seconds\n",
      "\n",
      "**CREATING FITS HDU**\n",
      "the entire process took: 971.83 seconds\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    hdu, processed_wise_data, wise_avg, wise_var = main('181D', save = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0ef88b",
   "metadata": {},
   "source": [
    "## Testing bad code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "50fe2ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd = processed_wise_data[processed_wise_data['sigdec'] < 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "51251c32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pwd = pwd[pwd['sigra'] < 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "17e40c5e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**PROCESSING DATAFRAME**\n",
      "\n",
      "**dataframe recieved**\n",
      "**dataframe sorted**\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paiaa\\anaconda3\\Lib\\site-packages\\numpy\\lib\\function_base.py:2412: RuntimeWarning: invalid value encountered in <lambda> (vectorized)\n",
      "  outputs = ufunc(*inputs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**relevant columns added**\n",
      "**data filtered**\n",
      "processing object 0 out of 11273\n",
      "processing object 2500 out of 11273\n",
      "processing object 5000 out of 11273\n",
      "processing object 7500 out of 11273\n",
      "processing object 10000 out of 11273\n",
      "**mean, variance calculated**\n",
      "**epochs labelled**\n",
      "this took: 2927.85 seconds\n",
      "\n",
      "**PERFORMING STATISTICAL CALCULATIONS**\n",
      "**finished calculating distribution of observations**\n",
      "**finished calculating within epoch statistics**\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paiaa\\AppData\\Local\\Temp\\ipykernel_10236\\184507825.py:18: RuntimeWarning: Mean of empty slice\n",
      "  var = np.nanmean(array)/array[~np.isnan(array)].shape[0]\n",
      "C:\\Users\\paiaa\\AppData\\Local\\Temp\\ipykernel_10236\\184507825.py:28: RuntimeWarning: Mean of empty slice\n",
      "  var = np.nanmean(array)/array[~np.isnan(array)].shape[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**finished calculating epoch to epoch statistics**\n",
      "this took: 70.71 seconds\n",
      "\n",
      "**CREATING FITS HDU**\n",
      "the entire process took: 2998.64 seconds\n"
     ]
    }
   ],
   "source": [
    "t1 = time.time()\n",
    "pwd_final, pwd_avg, pwd_var = process_df(pwd, '181D')\n",
    "stat_data = create_fits_data(pwd_final, pwd_avg, pwd_var)\n",
    "hdu2 = create_fits_file(stat_data)\n",
    "\n",
    "\n",
    "#save_data(hdu2, pwd_final, pwd_avg, pwd_var)\n",
    "\n",
    "t2 = time.time()\n",
    "print('the entire process took: ' + str(round(t2-t1, 2)) + ' seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "e7dcae76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<astropy.io.fits.hdu.table.BinTableHDU at 0x1fb171674d0>"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hdu2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "7996c89f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'manga_hdu' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[149], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m manga_hdu\n",
      "\u001b[1;31mNameError\u001b[0m: name 'manga_hdu' is not defined"
     ]
    }
   ],
   "source": [
    "manga_hdu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e294555",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hdu_manga = manga_file[1]\n",
    "manga_data = hdu_manga.data\n",
    "test = processed_wise_data.groupby(pd.Grouper(key = 'plateifu'), dropna = False).size().reset_index(name = 'count')\n",
    "manga_ra = np.array([])\n",
    "manga_dec = np.array([])\n",
    "\n",
    "wise_ra = np.array([])\n",
    "wise_dec = np.array([])\n",
    "\n",
    "for pifu in test['plateifu']:\n",
    "    manga_ra = np.append(manga_ra, manga_data[manga_data['plateifu'] == pifu]['objra'])\n",
    "    manga_dec = np.append(manga_dec, manga_data[manga_data['plateifu'] == pifu]['objdec'])\n",
    "    \n",
    "    wise_ra = np.append(wise_ra, processed_wise_data[processed_wise_data['plateifu'] == pifu]['ra'].values[0])\n",
    "    wise_dec = np.append(wise_dec, processed_wise_data[processed_wise_data['plateifu'] == pifu]['dec'].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "c3dd47bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>plateifu</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10001-12701</td>\n",
       "      <td>288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10001-12702</td>\n",
       "      <td>281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10001-12703</td>\n",
       "      <td>264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10001-12704</td>\n",
       "      <td>291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10001-12705</td>\n",
       "      <td>276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10747</th>\n",
       "      <td>9894-6102</td>\n",
       "      <td>303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10748</th>\n",
       "      <td>9894-6103</td>\n",
       "      <td>321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10749</th>\n",
       "      <td>9894-6104</td>\n",
       "      <td>322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10750</th>\n",
       "      <td>9894-9101</td>\n",
       "      <td>329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10751</th>\n",
       "      <td>9894-9102</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10752 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          plateifu  count\n",
       "0      10001-12701    288\n",
       "1      10001-12702    281\n",
       "2      10001-12703    264\n",
       "3      10001-12704    291\n",
       "4      10001-12705    276\n",
       "...            ...    ...\n",
       "10747    9894-6102    303\n",
       "10748    9894-6103    321\n",
       "10749    9894-6104    322\n",
       "10750    9894-9101    329\n",
       "10751    9894-9102     15\n",
       "\n",
       "[10752 rows x 2 columns]"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "994eb19f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10         10001-12701\n",
       "8          10001-12701\n",
       "0          10001-12701\n",
       "2          10001-12701\n",
       "4          10001-12701\n",
       "              ...     \n",
       "3030812      9894-9102\n",
       "3030815      9894-9102\n",
       "3030805      9894-9102\n",
       "3030808      9894-9102\n",
       "3030807      9894-9102\n",
       "Name: plateifu, Length: 3058410, dtype: object"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_wise_data[['plateifu']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "a1b5c09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "manga_file = fits.open(r\"C:\\Users\\paiaa\\Blanton Lab\\mnsa-0.3.0.fits\")\n",
    "hdu_manga = manga_file[2]\n",
    "manga_data = hdu_manga.data\n",
    "RA = manga_file[1].data['objra']\n",
    "dec = manga_file[1].data['objdec']\n",
    "\n",
    "table = Table([RA, dec], names=('ra', 'dec'))\n",
    "table.write('objects_all.tbl', format = 'ipac', overwrite = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "b9fc8e03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "133.6855634"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_wise_data[processed_wise_data['plateifu'] == pifu]['ra'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7137c780",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
